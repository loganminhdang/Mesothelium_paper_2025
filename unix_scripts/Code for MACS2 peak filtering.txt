#!/bin/bash
#
# ==============================================================================
#
#             Iterative ATAC-seq Peak Filtering and Processing Pipeline
#
# ==============================================================================
#
# Author:  Quang Dang
# Date: 2025-08-02
# Description: This script processes ATAC-seq peak files from MACS2. It performs
#              summit extension, iterative filtering to remove weak peaks, score
#              normalization, and finally merges peaks from multiple samples into
#              a consensus peak set. Heavily inspired by Corces et al. (2018). 
#
# Requirements:
#   - micromamba
#   - bedtools
#   - bedops
#   - GNU Parallel
#   - Custom scripts:
#     - calculate_center_peaks_and_extend.sh
#     - iterative_peak_filtering.sh
#     - normalize_macs2_peak_scores.sh
#
# Usage:
#   - Ensure the 'bioinfo' micromamba environment is set up with all dependencies.
#   - Place this script in the same directory as the custom helper scripts.
#   - Update the file paths and sample IDs in the "Configuration" section.
#   - Execute the script from your terminal: ./run_peak_filtering.sh
#
# ==============================================================================

# --- Configuration ---

# Activate the required micromamba environment
echo "Activating 'bioinfo' environment..."
eval "$(micromamba shell hook --shell bash)"
micromamba activate bioinfo

# Set a temporary directory for GNU parallel to use
export TMPDIR="/mnt/sdb/qdang/tmp"
mkdir -p "${TMPDIR}"

# --- Set Base Directories ---
# The directory where this script and the helper scripts are located
SCRIPTS_DIR="/home/qdang/atac/iterative_peak_filtering"
# The main directory for MACS2 peak calling results
PEAK_DIR="/mnt/sdb/qdang/atac/macs2_peakcalling/run2_652024/q_001"

# --- Set Core File Paths ---
# A file containing one sample ID per line (e.g., E115.mRp.clN)
SAMPLE_IDS_FILE="/mnt/sdb/qdang/atac/ids.txt"
# Chromosome sizes file, required by helper scripts
CHROM_SIZES="mm10_chrom_sizes_scaffold.txt"

# --- Navigate to the script directory ---
# This ensures that the helper scripts can be found and executed.
cd "${SCRIPTS_DIR}" || { echo "ERROR: Could not change to script directory: ${SCRIPTS_DIR}"; exit 1; }

# ==============================================================================
#
#        Part 1: Per-Sample Peak Processing
#
# ==============================================================================
echo "Starting Part 1: Processing peaks for each sample..."

# --- Step 1.1: Extend peak summits ---
# For each sample, find the center of the MACS2 summit and extend it by 250bp
# in each direction to create a uniform 501bp peak region.
echo "Extending peak summits to 501bp..."
cat "${SAMPLE_IDS_FILE}" | parallel --tmpdir $TMPDIR \
    ./calculate_center_peaks_and_extend.sh \
        "${PEAK_DIR}/{}_summits.bed" \
        "${PEAK_DIR}/{}_summits.extended.bed" \
        "${CHROM_SIZES}" \
        250

# --- Step 1.2: Iteratively filter peaks ---
# Remove peaks that overlap with stronger peaks to reduce redundancy and
# retain the most significant signal.
echo "Performing iterative filtering on each sample..."
cat "${SAMPLE_IDS_FILE}" | parallel --tmpdir $TMPDIR \
    ./iterative_peak_filtering.sh \
        "${PEAK_DIR}/{}_summits.extended.bed" \
        "${PEAK_DIR}/{}_summits.extended.filtered.bed" \
        "${CHROM_SIZES}"

# --- Step 1.3: Normalize peak scores ---
# Normalize the MACS2 scores (e.g., by total signal in peaks) to make them
# comparable across different samples.
echo "Normalizing peak scores..."
cat "${SAMPLE_IDS_FILE}" | parallel --tmpdir $TMPDIR \
    ./normalize_macs2_peak_scores.sh \
        "${PEAK_DIR}/{}_summits.extended.filtered.bed" \
        "${PEAK_DIR}/{}_summits.extended.filtered.normalized.bed"

# ==============================================================================
#
#        Part 2: Merge and Finalize Consensus Peak Set
#
# ==============================================================================
echo "Starting Part 2: Merging peaks into a consensus set..."

# --- Step 2.1: Merge peaks from all samples ---
# Combine the filtered, normalized peak sets from all time points into a
# single master file. bedops -u (union) is used for this purpose.
# NOTE: Update the file paths here if your sample IDs change.
MERGED_PEAK_FILE="${PEAK_DIR}/merged.bed"
echo "Merging normalized peaks from all samples..."
bedops -u \
    "${PEAK_DIR}/E115.mRp.clN_summits.extended.filtered.normalized.bed" \
    "${PEAK_DIR}/E135.mRp.clN_summits.extended.filtered.normalized.bed" \
    "${PEAK_DIR}/E175.mRp.clN_summits.extended.filtered.normalized.bed" \
    > "${MERGED_PEAK_FILE}"

# --- Step 2.2: Perform final iterative filtering ---
# Run the iterative filtering one last time on the merged set to resolve any
# remaining overlaps and create the final, high-confidence consensus peak set.
FINAL_PEAK_FILE="${PEAK_DIR}/merged.filtered.bed"
echo "Performing final filtering on the merged peak set..."
./iterative_peak_filtering.sh \
    "${MERGED_PEAK_FILE}" \
    "${FINAL_PEAK_FILE}" \
    "${CHROM_SIZES}"

# ==============================================================================
echo "Peak processing complete."
echo "Final consensus peak set is located at: ${FINAL_PEAK_FILE}"
# ==============================================================================